# Seminar Instructions ----------------------------------------------------

# • ip: ip address of click.
# • app: app id for marketing.
# • device: device type id of user mobile phone (e.g., iphone 6 plus, iphone 7, huawei mate 7, etc.)
# • os: os version id of user mobile phone
# • channel: channel id of mobile ad publisher
# • click_time: timestamp of click (UTC)
# • attributed_time: if user download the app for after clicking an ad, this is the time of the app download
# • is_attributed: the target that is to be predicted, indicating the app was downloaded

# GENERAL DESCRIPTION: Detecting Click Fraud with the help of Naïve Bayes
# Online-, mobile-, and social media advertising have seen a substantial increase within the last decade. The immense growth has unfortunately also attracted many dark players who use bots, click networks, and other fraud techniques to steal advertising money by increasing clicks. Advertisers around the globe therefore invest substantial resources in understanding, which clicks they receive are fraudulent (not resulting in conversions) and which are real (having at least a likelihood to turn into a conversion). Machine Learning techniques such as e.g. Naïve Bayes Techniques are a helpful tool to encounter click-fraud. Using large amounts of training data, these algorithms seek to understand which factors are suitable to predict if a click origins from a real consumer or from a fraudulent operator.
# Students are required to give a full overview of the different Naïve Bayes Models available and need to discuss the different approaches available for these techniques.
# Students shall further identify, systematize, and compare current applications of Naïve Bayes Models in marketing research and marketing practice (beside click fraud identification).
# Finally, students are required to demonstrate with the help of the provided data and the click-fraud case how the different types and forms of Naïve Bayes models work and which type of model is most suited for the given case. Students are especially required to use the available training data to develop a model that performs best possible predicting the test data. Students are further required to have a close eye on possible overfitting-issues.

#Kick-Off Information
# • More than 35% of all clicks online are generated by bots
# • Still 65% of online ads are based on Costs-Per-Click models
# • Fraudulent clicks are thus a substantial problem in the online advertising world
# • Understanding which clicks really lead to conversion essential to sort the wheat from the chaff
# • Data Set with Clicks from an In-App-Advertising Campaign
# • Data features information about click-time, user ip, click source, conversion, OS, user device,
# and industry channel
# • More than 5 million observations for training
# • 200,000 observations for forecast

#Information regarding features 



# 1) Data Import ---------------------------------------------------------------

#install.packages("readr")
library(readr)

setwd("C:\\Users\\nilsb\\sciebo\\Master\\2. Semester\\Seminar\\Projekt\\Data")

#Load initial dataset
# install.packages("data.table")
# install.packages("tibble")
library(data.table)
library(tibble)
back_up = as.tibble(fread("train-all.csv", na.strings = ""))
dataset = as.tibble(fread("train-all.csv", na.strings = ""))
head(dataset)

# Convert the variables "ip", "app", "device", "os", "channel" & the target variable "is_attributed" into factors 
convert_features <- c("ip", "app", "device", "os", "channel", "is_attributed")
dataset[convert_features] <- lapply(dataset[convert_features], factor)
head(dataset)

#Convert th variables "click_time" & "attributed_time" into POSIXct format
# install.packages("fasttime")
library(fasttime)
dataset$click_time = fastPOSIXct(dataset$click_time)
dataset$attributed_time = fastPOSIXct(dataset$attributed_time)
head(dataset)

#Load data which should be forecasted
data_forecast = as.tibble(fread("test-all.csv", na.strings = ""))
head(data_forecast)

# Convert the variables "click_id", "ip", "app", "device", "os" and "channel" into factors
convert_features_fc <- c("ip", "app", "device", "os", "channel")
data_forecast[convert_features_fc] <- lapply(data_forecast[convert_features_fc], factor)

# Convert "click_time" variable into POSIXct format
data_forecast$click_time = fastPOSIXct(data_forecast$click_time)

# Control if transformation of variables was successful 
head(data_forecast)

#Split complete dataset into two sets: Training & Test-set
# install.packages("ISLR")
library(ISLR)
attach(dataset)
smp_siz = floor(0.90*nrow(dataset))  # creates a value for dividing the data into train and test. In this case the value is defined as 90% of the number of rows in the dataset
smp_siz  # shows the value of the sample size

set.seed(456)   # set seed to ensure we always have same random numbers generated
train_ind = sample(seq_len(nrow(dataset)),size = smp_siz)  # Randomly identifies the rows equal to sample size ( defined in previous instruction) from  all the rows of dataset and stores the row number in train_ind
training_set=dataset[train_ind,] #creates the training dataset with row numbers stored in train_ind
test_set=dataset[-train_ind,]  # creates the test dataset excluding the row numbers mentioned in train_ind

# Check whether transformations have worked out and control the size of the training- & testset
str(training_set)
str(test_set)

# 2) Data Exploration --------------------------------------------------------
#install.packages("purrr")
library(purrr)

#a) Check NA's

# Check percentage of NA's for each feature for the whole dataset
na_per_feature = apply(dataset, 2, function(col) sum(is.na(col))/length(col))
na_per_feature
is.vector(na_per_feature)

#Check number of observations WITHOUT Click Fraud
percent_attributed_dataset = nrow(subset(dataset, dataset$is_attributed==1))/nrow(dataset)
percent_attributed_dataset
#Validate whether the sum the percent of the number of observations WITHOUT Click Fraud and the number of NA's per feature "attributed_time" (where is_attributed = 0) adds up to 1 and thus suggest the conclusion that NA's occur only when 

sum(percent_attributed_dataset, na_per_feature[7])

# Conclusion: NA's only occur in the feature "attributed_time" when "is_attributed"=0, meaning that the app was not downloaded and Click Fraud has happened. This observation does make sense intuitively, since "attributed_time" tells us when an app was downloaded. When an app was not downloaded (is_attributed=0), there can be no information about the download time ("attributed_time").
# We face systematic NA (not MAR). The missingness fully depends on "is_attributed".

#b) Visualize the Data

str(dataset)

#ip

#app

#device

#os

#channel

#click_time

#attributed_time

#is_attributed


## Basic Statistics of the two sets
# Examine Data
str(dataset)
summary(dataset)

hist(dataset$ip)
hist(dataset$os)

#b) Check interesting statistics

# Research Questions:
# 1. Which time does make it likely that Click Fraud has happened?
#   Is it more likely that Click Fraud has happenend during night times in comparison to during the day?
#   At which times of the day is it more likely that click fraud happens (morning, noon, afternoon, evening)?

# 2.  Which OS does make it most likely that Click Fraud has happened?
#   Can you filter out certain OS in conjunction with certain devices at which Click Fraud comes up more likely?

# 3.  Are their IP addresses which can be clearly assigned to a bot?
#   •   Is there a certain pattern in IP adresses based on those with click fraud?
#   Do those come from a certain country (e.g. where a certain proxy came from)?


##Check distribution of fraudulent and real clicks
plot(dataset$is_attributed)
#Check percentage of non-fraudulent clicks based on the dataset
percent_attributed_dataset = nrow(subset(dataset, dataset$is_attributed==1))/nrow(dataset)
percent_attributed_dataset
#Check absolute number of non-fraudulent clicks
nrow(subset(dataset, dataset$is_attributed==1))

#Check percentage of "real" clicks based on the dataset
percent_real_dataset = nrow(subset(dataset, dataset$is_attributed==0))/nrow(dataset)
percent_real_dataset
#Check absolute number of real clicks
nrow(subset(dataset, dataset$is_attributed==0))

#Get different unique OS
unique(dataset$os)
#Get number of different OS included in the Dataset
str(dataset$os)


# Check correlation between features
correlations = cor(training_set)
install.packages("xtable")
library(xtable)
#Check correlations between variables, which might by dependent 
cor(dataset$device, dataset$os)
cor(dataset$device, dataset$ip)
cor(dataset$channel, dataset$ip)

# install.packages("Hmisc")
library(Hmisc)
#Get correlations with sig. convert to matrix first
rcorr(as.matrix(dataset$ip), as.matrix(dataset$channel))

#Frage: Ist das Ergebnis nicht falsch, weil im Datensatz die einzelnen Column noch nicht in factor umgewandelt wurde, sondern alle noch Integer sind?

# 3) Data Preparation --------------------------------------------------------
library(dplyr)

#Verify that data is in a correct structure (dataframe/tibble) & check structure
class(training_set)
str(training_set)
summary(training_set)

class(test_set)
str(test_set)
summary(test_set)


# a) Take care of NA - Even necessarry with Naive Bayes?

    # Column "attributed_time"
    # Maybe use seperate Naive Bayes Classifier in order to classify the NA's
        # Wird wahrscheinlich aber nicht möglich sein, weil Time variable NA's hat und damit viel zu viele Klassen, die predicted werden müssten

## Binning 
# Apply on the variables "click_time" and "attributed_time" - POSIXct format

?cut.POSIXt
?cut

# x <- as.POSIXct("2016-01-01 00:00:00", tz="UTC") + as.difftime(30*(0:47),units="mins")
# cut(x, breaks="2 hours", labels=FALSE)
# # or to show more clearly the results:
# data.frame(x, cuts = cut(x, breaks="2 hours", labels=FALSE))


#Evtl auch binning in Stunden und Dates machen, ohne Minuten und Sekunden - aber nochmal abgleichen mit Data Preprocessing in Data Mining Buch


# Remove highly correlated variables



# 4) Modeling ----------------------------------------------------------------

library(randomForest)
randomForest(is_attributed~., data=training_set, ntree=500)
?randomForest

#Method 1 -----------------
set.seed(123)
# install.packages('e1071')
library(e1071)
classifier = naiveBayes(x = training_set[-8],
                        y = training_set$is_attributed)
classifier = naiveBayes(is_attributed ~ ., data= training_set)

class(classifier)
summary(classifier) #Summary gives conditional probabilities across all features

print(classifier)

# test_set$y_pred = predict(classifier, newdata = test_set)
y_pred = predict(classifier, newdata = test_set[-8])
table(y_pred)  #Inspiration: table(predictions_mlr[,1],Titanic_dataset$Survived)

# Making the Confusion Matrix
cm = table(test_set[, 8], y_pred)

# Method 2 -----------------

set.seed(987)

# Alternative Package
# install.packages("naivebayes")
library(naivebayes)

# Build a new model using the Laplace correction
classfifier_2 <- naive_bayes(is_attributed ~ ., data=training_set, laplace=1)

# Observe the new predicted probabilities for a weekend afternoon
y_predict <- predict(locmodel2, newdata=test_set[-8], type = "prob")


# 5.  Inference: Which feature does help most to predict whether Click Fraud has happened or not?
#   Is it really one feature or a combination of different features?

# 7.  How fast does the Naive Bayes Classifier calculate the result? Is in that time a major advantage in comparison to other algorithms? What exactly do we have to measure in order to check the time - prediction function of the classifier on the data_forecast set?

# 9.  Kann man die Observations rausfiltern, bei denen die Wahrscheinlichkeit der Zuordnung zu einer Klasse nur marginal höher ist als die Zuordnung der anderen Klasse (z.B. 0.51 Klasse 1 vs. 0.49 für Klasse 2? Gibt es hierfür bestimmte Parameter, die man noch angeben kann?
#     •   P(Click Fraud | All Features) vs. P(Click Fraud | only a certain feature)
  



# 5) Model Performance -------------------------------------------------------

apply(dataset)
install.packages("MLmetrics")
library(MLmetrics)

# Training / Test Split
# k-fold Cross Validation

# a) False Positives & False Negatives
# b) F1 Score
# c) ROC Curve

# d) Confusion Matrix
# e) CAP Curve

# e) Gini Coefficient (?)

## Model Performance - Basic

## Model Performance - Extensions


# 6) Data Visualization ------------------------------------------------------

#https://datascienceplus.com/machine-learning-results-one-plot-to-rule-them-all/
# Check different plots to visualize classification results

# Comparison with Performance of other Algorithms -------------------------

# a) CART 
# install.packages("ranger")
library(ranger)

# b) Random Forest
# install.packages("randomForest")
library(randomForest)

# c) Boosted Trees - XGBOOST
# install.packages("xgboost")
library(xgboost)

# d) SVM
library(e1071)


# 7) Model Performance Comparison  -------------------------------------------


# Sample Code Udemy -------------------------------------------------------------

#Sample Code for Naive Bayes for DIFFERENT problem

# # Fitting SVM to the Training set

# classifier = naiveBayes(x = training_set[-3],
#                         y = training_set$Purchased)
# 
# # Predicting the Test set results
# y_pred = predict(classifier, newdata = test_set[-3])
# 
# # Making the Confusion Matrix
# cm = table(test_set[, 3], y_pred)
# 
# # Visualising the Training set results
# library(ElemStatLearn)
# set = training_set
# X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
# X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
# grid_set = expand.grid(X1, X2)
# colnames(grid_set) = c('Age', 'EstimatedSalary')
# y_grid = predict(classifier, newdata = grid_set)
# plot(set[, -3],
#      main = 'SVM (Training set)',
#      xlab = 'Age', ylab = 'Estimated Salary',
#      xlim = range(X1), ylim = range(X2))
# contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
# points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
# points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 
# # Visualising the Test set results
# library(ElemStatLearn)
# set = test_set
# X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
# X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
# grid_set = expand.grid(X1, X2)
# colnames(grid_set) = c('Age', 'EstimatedSalary')
# y_grid = predict(classifier, newdata = grid_set)
# plot(set[, -3], main = 'SVM (Test set)',
#      xlab = 'Age', ylab = 'Estimated Salary',
#      xlim = range(X1), ylim = range(X2))
# contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
# points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
# points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))


# Sample Code DataCamp ----------------------------------------------------

# building a Naive Bayes model
# install.packages("naivebayes")
library(naivebayes)

m <- naive_bayes(location ~ time_of_day, data = location_history)

# making predictions with Naive Bayes
future_location <- predict(m, future_conditions)

# The 'naivebayes' package is loaded into the workspace
# and the Naive Bayes 'locmodel' has been built

# Examine the location prediction model
locmodel

# Obtain the predicted probabilities for Thursday at 9am
predict(locmodel, thursday9am , type = "prob")

# Obtain the predicted probabilities for Saturday at 9am
predict(locmodel, saturday9am, type="prob")

# The 'naivebayes' package is loaded into the workspace already

# Build a NB model of location
locmodel <- naive_bayes(location ~ daytype + hourtype, data=locations)

# Predict Brett's location on a weekday afternoon
predict(locmodel, weekday_afternoon)

# Predict Brett's location on a weekday evening
predict(locmodel, newdata=weekday_evening)

#Great job! Your Naive Bayes model forecasts that Brett will be at the office on a weekday afternoon and at home in the evening.

?naive_bayes

# The 'naivebayes' package is loaded into the workspace already
# The Naive Bayes location model (locmodel) has already been built

# Observe the predicted probabilities for a weekend afternoon
predict(locmodel, weekend_afternoon, type = "prob")

# Build a new model using the Laplace correction
locmodel2 <- naive_bayes(location ~ daytype + hourtype, data=locations, laplace=1)

# Observe the new predicted probabilities for a weekend afternoon
predict(locmodel2, weekend_afternoon, type = "prob")


# Sample Code R for Marketing Book ----------------------------------------

# set.seed(04625) 
# train.prop <- 0.65 
# train.cases <- sample(nrow(seg.raw), nrow(seg.raw)*train.prop) 
# seg.df.train <- seg.raw[train.cases, ] 
# seg.df.test <- seg.raw[-train.cases, ]

library(e1071)
(initial_classified <- naiveBayes(is_attributed~., data=training_set))


# Sample Code R bloggers --------------------------------------------------

# library (e1071)
# ?naiveBayes
# data ( "Titanic" )
# Titanic_df= as.data.frame (Titanic)
# repeating_sequence= rep.int ( seq_len ( nrow (Titanic_df)), Titanic_df$Freq)
# Titanic_dataset=Titanic_df[repeating_sequence,]
# Titanic_dataset$Freq= NULL
# Naive_Bayes_Model= naiveBayes (Survived ~., data=Titanic_dataset)
# Naive_Bayes_Model
# NB_Predictions= predict (Naive_Bayes_Model,Titanic_dataset)
# table (NB_Predictions,Titanic_dataset$Survived)
# library (mlr)
# task = makeClassifTask (data = Titanic_dataset, target = "Survived" )
# selected_model = makeLearner ( "classif.naiveBayes" )
# NB_mlr = train (selected_model, task)
# NB_mlr$learner.model
# predictions_mlr = as.data.frame ( predict (NB_mlr, newdata = Titanic_dataset[,1:3]))
# table(predictions_mlr[,1],Titanic_dataset$Survived)


# Best Practices ----------------------------------------------------------

# - Implement simple version of algorithm first
# - Do not necessarily apply 80/20 split