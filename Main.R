# Seminar Instructions ----------------------------------------------------

# GENERAL DESCRIPTION: Detecting Click Fraud with the help of Naïve Bayes
# Online-, mobile-, and social media advertising have seen a substantial increase within the last decade. The immense growth has unfortunately also attracted many dark players who use bots, click networks, and other fraud techniques to steal advertising money by increasing clicks. Advertisers around the globe therefore invest substantial resources in understanding, which clicks they receive are fraudulent (not resulting in conversions) and which are real (having at least a likelihood to turn into a conversion). Machine Learning techniques such as e.g. Naïve Bayes Techniques are a helpful tool to encounter click-fraud. Using large amounts of training data, these algorithms seek to understand which factors are suitable to predict if a click origins from a real consumer or from a fraudulent operator.
# Students are required to give a full overview of the different Naïve Bayes Models available and need to discuss the different approaches available for these techniques.
# Students shall further identify, systematize, and compare current applications of Naïve Bayes Models in marketing research and marketing practice (beside click fraud identification).
# Finally, students are required to demonstrate with the help of the provided data and the click-fraud case how the different types and forms of Naïve Bayes models work and which type of model is most suited for the given case. Students are especially required to use the available training data to develop a model that performs best possible predicting the test data. Students are further required to have a close eye on possible overfitting-issues.

#Kick-Off Information
# • More than 35% of all clicks online are generated by bots
# • Still 65% of online ads are based on Costs-Per-Click models
# • Fraudulent clicks are thus a substantial problem in the online advertising world
# • Understanding which clicks really lead to conversion essential to sort the wheat from the chaff
# • Data Set with Clicks from an In-App-Advertising Campaign
# • Data features information about click-time, user ip, click source, conversion, OS, user device,
# and industry channel
# • More than 5 million observations for training
# • 200,000 observations for forecast

#Information regarding features 


# Open Questions  ----------------------------------------------

# 1) Which time does make it likely that Click Fraud has happened?
    # Is it more likely that Click Fraud has happenend during night times in comparison to during the day?
    # At which times of the day is it more likely that click fraud happens (morning, noon, afternoon, evening)?
# 2) Which OS does make it most likely that Click Fraud has happened?
    # Can you filter out certain OS in conjunction with certain devices at which Click Fraud comes up more likely?
# 3) Are their IP addresses which can be clearly assigned to a bot?
    # Is there a certain pattern in IP adresses based on those with click fraud?
      # Do those come from a certain country (e.g. where a certain proxy came from)?
# 4) Does the time in which data is generated bias the outcomes of the algorithms? (Only three day period after all)
    # What time period is considered within the data?
      # Only three days 
    # Although we have 5 million rows of data - it might be that this data is still not that representative given that it comprises only 3 days of records?
      # Maybe bots are rather active at certain days within a week or regarding a certain time in the month/quarter/year etc.
# 5) Inference: Which feature does help most to predict whether Click Fraud has happened or not?
    # Is it really one feature or a combination of different features?
# 6) How to build a pipeline classifier? When setting bins, new data is sorted into those bins so that classification works.
    # Also how do newly created features fit automatically to a new variable which is created on basis of known interaction effect?
# 7) How fast does the Naive Bayes Classifier calculate the result? Is in that time a major advantage in comparison to other algorithms? What exactly do we have to measure in order to check the time - prediction function of the classifier on the data_forecast set?
    # Also research the framing of click fraud problems. Is time here a major factor? Or is it more important that the classifier is more accurate and the frauds can be identified also some other after the click fraud has happened?
      # Oder wird beim Click Frauf direkt abgerechnet an den Bewerbenden, sodass eine direkte Erkennung notwendig ist? 
# 8) Was für coole (evtl. interaktive) Visualisierungstechniken gibt es, um die Ergebnisse darzustellen?
    #CAP Curve
    #POC Curve
    #COntingency Matrix 
# 9) Kann man die Observations rausfiltern, bei denen die Wahrscheinlichkeit der Zuordnung zu einer Klasse nur marginal höher ist als die Zuordnung der anderen Klasse (z.B. 0.51 Klasse 1 vs. 0.49 für Klasse 2? Gibt es hierfür bestimmte Parameter, die man noch angeben kann?
    # P(Click Fraud | All Features) vs. P(Click Fraud | only a certain feature)
# 10) Wie stark können wir davon ausgehen, dass die einzelnen Features wirklich independent sind? Ist es nicht wahrscheinlich, dass bestimmte Botnetzwerke zwar die IP-Adresse häufig ändern, aber trotzdem das gleiche Gerät nutzen bzw. das gleiche OS? -> Das würde die Assumption der Unabhängigkeit der Features schaden.
    # Ja, ist wahrscheinlich. Eventuell trotzdem Basic Naive Bayes Classifier ausprobieren und Ergebnisse des Models checken. Trotzdem auch nochmal Extensions anschauen und überlegen, wie Naive Bayes damit performt (ADA Boost, Interaction Effect, verschiedene Verteilungen, Laplace, etc.)?
# 11) Wie kann man Overfitting bei Naive Bayes verhindern? Ist es nicht eher so, dass der Naive Bayes eher dazu tendiert underfitting zu machen und deshalb das Thema (außer man wendet beispielsweise Boosting in Kombi mit Naive Bayes an) vernachlässigbar ist primär?
# 12) Welche Key Performance Metric sollte man für das spezifische Problem hier wählen?
  #Accuracy wahrscheinlich nicht ntuzen, weil target variable classes (natural & click fraud) sehr unausgeglichen sind (Hypothese - muss noch validiert werden).
  #F1 Score 
  #CAP Curve
# 13) Kann man hoch korrelierte Features eventuell zu einem neuen Feature zusammenfassen? Oder würde das dann eher die Informationen verwässern und die tatsächliche Click Fraud Erkennung später nicht möglich machen, weil das neue Feature in der Ausprägung ja nicht beobachtet wird, sondern eher die einzelnen Features, die in das gemeinsame neu erstellte Feature mit reinfließen würden?
# 14) Brainstorming welche Interaktionskombinationen zwischen den einzelnen Features realistisch sein können.
  #(z.B. app, OS, device)
  #OS und Device - sehr wahrscheinlich
#15) Wenn man eine neue Variable als Interaktion erstellt, kann es sein, dass man dann Probleme damit bekommt das forecasted set darauf zu basieren?
#16) Ist nach Erläuterung der einzelnen Feature das Problem eigentlich nicht eher ein Problem, dass manipulativ eine App heruntergeladen wird und NICHT dass eine Display Ad falscherweise heruntergeladen wurde?
# 17) Wie schneidet Algorithmus ab wenn man hoch korrelierte Variablen drin lässt vs. rauslöscht vs. neue zusammenfassende Interaktionsvariable einbaut und die alten interagierenden Variablen beim Model Fitting rauslässt?
# 18) Inhaltliche Frage zu der Interpretation des Outputs des Models: Werden einzelne Conditional Probabilities pro Feature ausgegeben? Kriegt man auch heraus, dass wenn die Daten in den Features einen bestimmten Wert annehmen, dasss das dann eher zur Zuordnung von y=1 führt währenddessen die Ausprägung bei anderen Features eher zur Zuordnung von y=0 führt.
# 19) Gibt es Ausprägungen von kategorischen Variablen im Test-set (to be forecasted), die es noch nicht im Training-Set gegeben hat? (In unserem Datenset kann das tatsächlich gut sein, wenn es ein zusätzliches Gerät bzw. zusätzliche Betriebssystemsversion gibt)
    # Sind diese Ausprägungen stark?
      # Hier mal Matching zwischen den spezifischen Ausprägungen zwischen Training & Testset machen + auch noch für forecast_dataset machen. 
        # Anzahl von Factor levels anschauen und checken ob diese vergleichbar sind und zusätzlich noch gucken wie viele jeweils gleich sind und wie viele nicht (Prozentzahl der Überschneidung ausrechnen und dann gucken, wie groß der prozentuale Anteil der Nicht-Überschneidung ist und checken, ob das dramatisch ist)
# 20) Können Korrelationen überhaupt gecheckt werden, wenn variables, noch als Integer drin stehen und noch nicht zu Faktoren umgeswitcht worden sind?
    #Frage: Ist das Ergebnis nicht falsch, weil im Datensatz die einzelnen Column noch nicht in factor umgewandelt wurde, sondern alle noch Integer sind?
      # Vielleicht chi-square oder ANOVA anzuwenden. https://stats.stackexchange.com/questions/108007/correlations-with-unordered-categorical-variables
      # Allgemein nochmal nachgucken, was hier bei Naive Bayes gefragt ist, um die Independence assumption abzuprüfen


# 1) Data Import ---------------------------------------------------------------

#install.packages("readr")
library(readr)

setwd("C:\\Users\\nilsb\\sciebo\\Master\\2. Semester\\Seminar\\Projekt\\Data")

#Load initial dataset
# install.packages("data.table")
# install.packages("tibble")
library(data.table)
library(tibble)
dataset = as.tibble(fread("train-all.csv", na.strings = ""))
head(dataset)

# Encoding the target feature as factor
dataset$is_attributed = factor(dataset$is_attributed, levels = c(0, 1))

#Convert dataset$click_time & dataset$attributed_time variables into POSIXct format
# install.packages("fasttime")
library(fasttime)
dataset$click_time = fastPOSIXct(dataset$click_time)
dataset$attributed_time = fastPOSIXct(dataset$attributed_time)
head(dataset)

#Split complete data set into two sets: Training & Test-set
# install.packages("ISLR")
library(ISLR)
attach(dataset)
smp_siz = floor(0.90*nrow(dataset))  # creates a value for dividing the data into train and test. In this case the value is defined as 90% of the number of rows in the dataset
smp_siz  # shows the value of the sample size

set.seed(456)   # set seed to ensure you always have same random numbers generated
train_ind = sample(seq_len(nrow(dataset)),size = smp_siz)  # Randomly identifies the rows equal to sample size ( defined in previous instruction) from  all the rows of dataset and stores the row number in train_ind
training_set=dataset[train_ind,] #creates the training dataset with row numbers stored in train_ind
test_set=dataset[-train_ind,]  # creates the test dataset excluding the row numbers mentioned in train_ind

head(training_set)
head(test_set)

#Load data which should be forecasted
data_forecast = as.tibble(fread("test-all.csv", na.strings = ""))
head(data_forecast)

# 2) Data Preparation --------------------------------------------------------
library(dplyr)

#Verify that data is in a correct structure (dataframe/tibble) & check structure
class(training_set)
str(training_set)
glimpse(training_set)
summary(training_set)

class(test_set)
str(test_set)
summary(test_set)


# Transform variabels into factors (validate if this is needed for every variable)

# dataset$ip = as.factor(dataset$ip)
# dataset$app = as.factor(dataset$app)
# dataset$device = as.factor(dataset$device)
# dataset$os = as.factor(dataset$os)
# dataset$channel = as.factor(dataset$channel)
str(dataset)

## Adapt the Data Format 

# a) Take care of NA - Even necessarry with Naive Bayes?
# Column "attributed_time"
# Maybe use seperate Naive Bayes Classifier in order to classify the NA's
    # Wird wahrscheinlich aber nicht möglich sein, weil Time variable NA's hat und damit viel zu viele Klassen, die predicted werden müssten
dataset$attributed_time = ifelse

# a) Binning necesarry to convert variables

# b) Add ID column (?)

# c) Converting chr into factor (?)

# d) Remove highly correlated variables

# 3) Data Exploration --------------------------------------------------------
library(purrr)

## Basic Statistics of the two sets
# Examine Data
str(dataset)
summary(dataset)

hist(dataset$ip)
hist(dataset$os)


##Check distribution of fraudulent and real clicks
plot(dataset$is_attributed)
#Check percentage of fraudulent clicks based on the dataset
percent_fraud_dataset = nrow(subset(dataset, dataset$is_attributed==1))/nrow(dataset)
percent_fraud_dataset
#Check absolute number of fraudulent clicks
nrow(subset(dataset, dataset$is_attributed==1))

#Check percentage of "real" clicks based on the dataset
percent_real_dataset = nrow(subset(dataset, dataset$is_attributed==0))/nrow(dataset)
percent_real_dataset
#Check absolute number of real clicks
nrow(subset(dataset, dataset$is_attributed==0))

#Get different unique OS
unique(dataset$os)
#Get number of different OS included in the Dataset
str(dataset$os)


# Check correlation between features
correlations = cor(training_set)
install.packages("xtable")
library(xtable)
#Check correlations between variables, which might by dependent 
cor(dataset$device, dataset$os)
cor(dataset$device, dataset$ip)
cor(dataset$channel, dataset$ip)

# install.packages("Hmisc")
library(Hmisc)
#Get correlations with sig. convert to matrix first
rcorr(as.matrix(dataset$ip), as.matrix(dataset$channel))

#Frage: Ist das Ergebnis nicht falsch, weil im Datensatz die einzelnen Column noch nicht in factor umgewandelt wurde, sondern alle noch Integer sind?

# 4) Modeling ----------------------------------------------------------------
# 4.1) Basic ---------------------------------------------------------------

#Method 1 -----------------
set.seed(123)

# install.packages('e1071')
library(e1071)
classifier = naiveBayes(x = training_set[-8],
                        y = training_set$is_attributed)
classifier = naiveBayes(is_attributed ~ ., data= training_set)

class(classifier)
summary(classifier) #Summary gives conditional probabilities across all features

print(classifier)

# test_set$y_pred = predict(classifier, newdata = test_set)
y_pred = predict(classifier, newdata = test_set[-8])
table(y_pred)  #Inspiration: table(predictions_mlr[,1],Titanic_dataset$Survived)

# Making the Confusion Matrix
cm = table(test_set[, 8], y_pred)

# Method 2 -----------------

set.seed(987)

# Alternative Package
# install.packages("naivebayes")
library(naivebayes)

# Build a new model using the Laplace correction
classfifier_2 <- naive_bayes(is_attributed ~ ., data=training_set, laplace=1)

# Observe the new predicted probabilities for a weekend afternoon
y_predict <- predict(locmodel2, newdata=test_set[-8], type = "prob")

# 4.2) Extensions ---------------------------------------------------------

set.seed(987)

# Gaussian Naive Bayes - Wahrscheinlich nicht notwendig, da keine real-valued inputs, sondern eigentlich nur Factors

# Parameter Tuning - Siehe Springleaf und riesiger For Loop mit verschiedenen Parameters

# Interaction Effects in das Modell integriert


# 5) Model Performance -------------------------------------------------------

apply(dataset)
install.packages("MLmetrics")
library(MLmetrics)

# Training / Test Split
# k-fold Cross Validation

# a) False Positives & False Negatives
# b) F1 Score
# c) ROC Curve

# d) Confusion Matrix
# e) CAP Curve

# e) Gini Coefficient (?)

## Model Performance - Basic

## Model Performance - Extensions


# 6) Data Visualization ------------------------------------------------------

# Comparison with Performance of other Algorithms -------------------------

# a) CART 
# install.packages("ranger")
library(ranger)

# b) Random Forest
# install.packages("randomForest")
library(randomForest)

# c) Boosted Trees - XGBOOST
# install.packages("xgboost")
library(xgboost)

# d) SVM
library(e1071)


# 7) Model Performance Comparison  -------------------------------------------


# Sample Code Udemy -------------------------------------------------------------

#Sample Code for Naive Bayes for DIFFERENT problem

# # Fitting SVM to the Training set

# classifier = naiveBayes(x = training_set[-3],
#                         y = training_set$Purchased)
# 
# # Predicting the Test set results
# y_pred = predict(classifier, newdata = test_set[-3])
# 
# # Making the Confusion Matrix
# cm = table(test_set[, 3], y_pred)
# 
# # Visualising the Training set results
# library(ElemStatLearn)
# set = training_set
# X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
# X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
# grid_set = expand.grid(X1, X2)
# colnames(grid_set) = c('Age', 'EstimatedSalary')
# y_grid = predict(classifier, newdata = grid_set)
# plot(set[, -3],
#      main = 'SVM (Training set)',
#      xlab = 'Age', ylab = 'Estimated Salary',
#      xlim = range(X1), ylim = range(X2))
# contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
# points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
# points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 
# # Visualising the Test set results
# library(ElemStatLearn)
# set = test_set
# X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
# X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
# grid_set = expand.grid(X1, X2)
# colnames(grid_set) = c('Age', 'EstimatedSalary')
# y_grid = predict(classifier, newdata = grid_set)
# plot(set[, -3], main = 'SVM (Test set)',
#      xlab = 'Age', ylab = 'Estimated Salary',
#      xlim = range(X1), ylim = range(X2))
# contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
# points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
# points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))


# Sample Code DataCamp ----------------------------------------------------

# building a Naive Bayes model
# install.packages("naivebayes")
library(naivebayes)

m <- naive_bayes(location ~ time_of_day, data = location_history)

# making predictions with Naive Bayes
future_location <- predict(m, future_conditions)

# The 'naivebayes' package is loaded into the workspace
# and the Naive Bayes 'locmodel' has been built

# Examine the location prediction model
locmodel

# Obtain the predicted probabilities for Thursday at 9am
predict(locmodel, thursday9am , type = "prob")

# Obtain the predicted probabilities for Saturday at 9am
predict(locmodel, saturday9am, type="prob")

# The 'naivebayes' package is loaded into the workspace already

# Build a NB model of location
locmodel <- naive_bayes(location ~ daytype + hourtype, data=locations)

# Predict Brett's location on a weekday afternoon
predict(locmodel, weekday_afternoon)

# Predict Brett's location on a weekday evening
predict(locmodel, newdata=weekday_evening)

#Great job! Your Naive Bayes model forecasts that Brett will be at the office on a weekday afternoon and at home in the evening.

?naive_bayes

# The 'naivebayes' package is loaded into the workspace already
# The Naive Bayes location model (locmodel) has already been built

# Observe the predicted probabilities for a weekend afternoon
predict(locmodel, weekend_afternoon, type = "prob")

# Build a new model using the Laplace correction
locmodel2 <- naive_bayes(location ~ daytype + hourtype, data=locations, laplace=1)

# Observe the new predicted probabilities for a weekend afternoon
predict(locmodel2, weekend_afternoon, type = "prob")


# Sample Code R for Marketing Book ----------------------------------------

# set.seed(04625) 
# train.prop <- 0.65 
# train.cases <- sample(nrow(seg.raw), nrow(seg.raw)*train.prop) 
# seg.df.train <- seg.raw[train.cases, ] 
# seg.df.test <- seg.raw[-train.cases, ]

library(e1071)
(initial_classified <- naiveBayes(is_attributed~., data=training_set))


# Sample Code R bloggers --------------------------------------------------

# library (e1071)
# ?naiveBayes
# data ( "Titanic" )
# Titanic_df= as.data.frame (Titanic)
# repeating_sequence= rep.int ( seq_len ( nrow (Titanic_df)), Titanic_df$Freq)
# Titanic_dataset=Titanic_df[repeating_sequence,]
# Titanic_dataset$Freq= NULL
# Naive_Bayes_Model= naiveBayes (Survived ~., data=Titanic_dataset)
# Naive_Bayes_Model
# NB_Predictions= predict (Naive_Bayes_Model,Titanic_dataset)
# table (NB_Predictions,Titanic_dataset$Survived)
# library (mlr)
# task = makeClassifTask (data = Titanic_dataset, target = "Survived" )
# selected_model = makeLearner ( "classif.naiveBayes" )
# NB_mlr = train (selected_model, task)
# NB_mlr$learner.model
# predictions_mlr = as.data.frame ( predict (NB_mlr, newdata = Titanic_dataset[,1:3]))
# table(predictions_mlr[,1],Titanic_dataset$Survived)


# Best Practices ----------------------------------------------------------

# - Implement simple version of algorithm first
# - Do not necessarily apply 80/20 split