# !diagnostics

# Seminar Instructions ----------------------------------------------------

# • ip: ip address of click.
# • app: app id for marketing.
# • device: device type id of user mobile phone (e.g., iphone 6 plus, iphone 7, huawei mate 7, etc.)
# • os: os version id of user mobile phone
# • channel: channel id of mobile ad publisher
# • click_time: timestamp of click (UTC)
# • attributed_time: if user download the app for after clicking an ad, this is the time of the app download
# • is_attributed: the target that is to be predicted, indicating the app was downloaded

# GENERAL DESCRIPTION: Detecting Click Fraud with the help of Naïve Bayes
# Online-, mobile-, and social media advertising have seen a substantial increase within the last decade. The immense growth has unfortunately also attracted many dark players who use bots, click networks, and other fraud techniques to steal advertising money by increasing clicks. Advertisers around the globe therefore invest substantial resources in understanding, which clicks they receive are fraudulent (not resulting in conversions) and which are real (having at least a likelihood to turn into a conversion). Machine Learning techniques such as e.g. Naïve Bayes Techniques are a helpful tool to encounter click-fraud. Using large amounts of training data, these algorithms seek to understand which factors are suitable to predict if a click origins from a real consumer or from a fraudulent operator.
# Students are required to give a full overview of the different Naïve Bayes Models available and need to discuss the different approaches available for these techniques.
# Students shall further identify, systematize, and compare current applications of Naïve Bayes Models in marketing research and marketing practice (beside click fraud identification).
# Finally, students are required to demonstrate with the help of the provided data and the click-fraud case how the different types and forms of Naïve Bayes models work and which type of model is most suited for the given case. Students are especially required to use the available training data to develop a model that performs best possible predicting the test data. Students are further required to have a close eye on possible overfitting-issues.

#Kick-Off Information
# • More than 35% of all clicks online are generated by bots
# • Still 65% of online ads are based on Costs-Per-Click models
# • Fraudulent clicks are thus a substantial problem in the online advertising world
# • Understanding which clicks really lead to conversion essential to sort the wheat from the chaff
# • Data Set with Clicks from an In-App-Advertising Campaign
# • Data features information about click-time, user ip, click source, conversion, OS, user device,
# and industry channel
# • More than 5 million observations for training
# • 200,000 observations for forecast

#Information regarding features 




# 1) Data Import ---------------------------------------------------------------

#install.packages("readr")
library(readr)

setwd("C:\\Users\\nilsb\\sciebo\\Master\\2. Semester\\Seminar\\Projekt\\Data")

#Load initial dataset
# install.packages("data.table")
# install.packages("tibble")
library(data.table)
library(tibble)
back_up = as.tibble(fread("train-all.csv", na.strings = ""))
dataset = as.tibble(fread("train-all.csv", na.strings = ""))

# Convert the variables "ip", "app", "device", "os", "channel" & the target variable "is_attributed" into factors 
convert_features <- c("ip", "app", "device", "os", "channel", "is_attributed")
dataset[convert_features] <- lapply(dataset[convert_features], factor)

#Convert th variables "click_time" & "attributed_time" into POSIXct format
# install.packages("fasttime")
library(fasttime)
dataset$click_time = fastPOSIXct(dataset$click_time)
dataset$attributed_time = fastPOSIXct(dataset$attributed_time)

#Load data which should be forecasted
data_forecast = as.tibble(fread("test-all.csv", na.strings = ""))

# Convert the variables "click_id", "ip", "app", "device", "os" and "channel" into factors
convert_features_fc <- c("ip", "app", "device", "os", "channel")
data_forecast[convert_features_fc] <- lapply(data_forecast[convert_features_fc], factor)

# Convert "click_time" variable into POSIXct format
data_forecast$click_time = fastPOSIXct(data_forecast$click_time)

# Control if transformation of variables was successful 
head(data_forecast)
head(dataset)



# 2) Data Preparation --------------------------------------------------------
library(dplyr)


# a. Apply binning --------------------------------------------------------

# Separate date and time information into two variables and apply binning on them separately to prepare for Naive Bayes

# install.packages("tidyr")
library(tidyr)

# i. Separation of "click_time"
#Separate blick_time variable into "click_date" and "click_exact_time"
dataset = separate(dataset, col=click_time, into = c("click_date", "click_exact_time"), sep= " ")
str(dataset)

# Convert variables "click_date" and "click_exact_time" into right formats -> Date and Time 
# install.packages("lubridate")
library(lubridate)
dataset$click_date = ymd(dataset$click_date) #choose this for date

#Apply binning on "click_date" and "click_exact_time"

# Binning on click_data
dataset$bins_click_date = NA
dataset$bins_click_date = cut(dataset$click_date, breaks="1 day", labels=FALSE) #works fine if previously converted
# Convert bin variable into factor
dataset$bins_click_date = as.factor(dataset$bins_click_date)
dataset$bins_click_date = factor(dataset$bins_click_date, levels=c("1","2","3","4"))

# Binning on click_exact_time - 12 intervals
dataset$bins_click_exact_time = NA
dataset$bins_click_exact_time <- cut(strptime(dataset$click_exact_time, format = "%H:%M:%S"), breaks=strptime(c("00:00:00","02:00:00","04:00:00","06:00:00","08:00:00","10:00:00","12:00:00","14:00:00","16:00:00","18:00:00","20:00:00","22:00:00"), format= "%H:%M:%S"), labels = c("0-2","2-4","4-6","6-8","8-10","10-12","12-14","14-16","16-18","18-20","20-22"))
dataset$bins_click_exact_time = as.character(dataset$bins_click_exact_time)
ind = which(is.na(dataset$bins_click_exact_time))
dataset$bins_click_exact_time[ind] = "22-24"
# Convert bin variable into factor
dataset$bins_click_exact_time = as.factor(dataset$bins_click_exact_time)
dataset$bins_click_exact_time = factor(dataset$bins_click_exact_time, levels=c("0-2","2-4","4-6","6-8","8-10","10-12","12-14","14-16","16-18","18-20","20-22","22-24"))

head(dataset)
str(dataset)

# # #Test: Binning on click_exact_time - 24 intervals
# # # Binning on click_exact_time
# dataset$bins_click_exact_time2 = NA
# dataset$bins_click_exact_time2 <- cut(strptime(dataset$click_exact_time, format = "%H:%M:%S"),
#                                      breaks=strptime(c("00:00:00","01:00:00","02:00:00", "03:00:00", "04:00:00", "05:00:00","06:00:00", "07:00:00", "08:00:00","09:00:00", "10:00:00", "11:00:00","12:00:00", "13:00:00", "14:00:00","15:00:00", "16:00:00", "17:00:00","18:00:00", "19:00:00", "20:00:00","21:00:00", "22:00:00", "23:00:00"), format= "%H:%M:%S"),
#                                      labels = c("0-1","1-2","2-3","3-4","4-5","5-6","6-7","7-8","8-9","9-10","10-11","11-12","12-13","13-14","14-15","15-16","16-17","17-18","18-19","19-20","20-21","21-22", "22-23"))
# 
# dataset$bins_click_exact_time2 = as.character(dataset$bins_click_exact_time2)
# ind = which(is.na(dataset$bins_click_exact_time2))
# dataset$bins_click_exact_time2[ind] = "23-24"
# # # Convert bin variable into factor
# dataset$bins_click_exact_time2 = as.factor(dataset$bins_click_exact_time2)
# # dataset$bins_click_exact_time = factor(dataset$bins_click_exact_time2, levels=c("0-2","2-4","4-6","6-8","8-10","10-12","12-14","14-16","16-18","18-20","20-22","22-24"))


# b. Split into Training-/Testset ------------------------------------

# install.packages("ISLR")
library(ISLR)
attach(dataset)
smp_siz = floor(0.90*nrow(dataset))  # creates a value for dividing the data into train and test. In this case the value is defined as 90% of the number of rows in the dataset
smp_siz  # shows the value of the sample size

set.seed(456)   # set seed to ensure we always have same random numbers generated
train_ind = sample(seq_len(nrow(dataset)),size = smp_siz)  # Randomly identifies the rows equal to sample size ( defined in previous instruction) from  all the rows of dataset and stores the row number in train_ind
training_set=dataset[train_ind,] #creates the training dataset with row numbers stored in train_ind
test_set=dataset[-train_ind,]  # creates the test dataset excluding the row numbers mentioned in train_ind

# Check whether transformations have worked out and control the size of the training- & testset
str(training_set)
str(test_set)





# 3) Data Exploration --------------------------------------------------------
#install.packages("purrr")
library(purrr)

# a. Check NA's -----------------------------------------------------------

#Check if NA's occur
any(is.na(dataset))
sum(is.na(dataset))
which(is.na(dataset))

#Part of dataset without NA's
dim(dataset[complete.cases(dataset), ])
# alt: na.omit(dataset)

#Part of dataset with NA's
dim(dataset[!complete.cases(dataset), ])

# Check percentage of NA's for each feature for the whole dataset
na_per_feature = apply(dataset, 2, function(col) sum(is.na(col))/length(col))
na_per_feature
is.vector(na_per_feature)

#Check percentage of observations WITHOUT Click Fraud
percent_attributed_dataset = nrow(subset(dataset, dataset$is_attributed==1))/nrow(dataset)
percent_attributed_dataset
#Validate whether the sum the percent of the number of observations WITHOUT Click Fraud and the number of NA's per feature "attributed_time" (where is_attributed = 0) adds up to 1 and thus suggest the conclusion that NA's occur only when 

sum(percent_attributed_dataset, na_per_feature[7])

# Conclusion: NA's only occur in the feature "attributed_time" when "is_attributed"=0, meaning that the app was not downloaded and Click Fraud has happened. This observation does make sense intuitively, since "attributed_time" tells us when an app was downloaded. When an app was not downloaded (is_attributed=0), there can be no information about the download time ("attributed_time").
# We face systematic NA (not MAR). The missingness fully depends on "is_attributed".

#General

# b. Get Basic Statistics -------------------------------------------------------------

# Get different datasets based on fraudulent and natural clicks
ds_is_attributed_1 = subset(dataset, subset=dataset$is_attributed==1)
ds_is_attributed_0 = subset(dataset, subset=dataset$is_attributed==0)

library(dplyr)
summary(dataset)

#Get number of days considered in dataset based on bins created
number_of_days = ((length(unique(dataset$bins_click_time)))*2)/24
number_of_days

##Check distribution of fraudulent and real clicks
plot(dataset$is_attributed)

#Check percentage of non-fraudulent clicks based on the dataset
percent_natural = nrow(subset(dataset, dataset$is_attributed==1))/nrow(dataset)
percent_natural
#Check absolute number of non-fraudulent clicks
nrow(subset(dataset, dataset$is_attributed==1))

#Check percentage of fraudulent clicks based on the dataset
percent_fraud = nrow(subset(dataset, dataset$is_attributed==0))/nrow(dataset)
percent_fraud

#Check absolute number of fraudulent clicks
number_frauds = nrow(subset(dataset, dataset$is_attributed==0))

#Get number of unique values per feature
str(dataset)
number_ip = length(unique(dataset$ip))
number_apps = length(unique(dataset$app))
number_devices = length(unique(dataset$device))
number_os = length(unique(dataset$os))
number_channels = length(unique(dataset$channel))
  

# c. Explore each Feature --------------------------------------------------

# install.packages("ggplot2")
library(ggplot2)
str(dataset)

# I. ip
ggplot(data=dataset, aes(x=ip)) + geom_bar()


# 3.  Are their IP addresses which can be clearly assigned to a bot?
#   •   Is there a certain pattern in IP adresses based on those with click fraud?
#   Do those come from a certain country (e.g. where a certain proxy came from)?

# II. app
ggplot(data=dataset, aes(x=app)) + geom_bar()

# III. device
ggplot(data=dataset, aes(x=device)) + geom_bar()

# IV. os

# 2.  Which OS does make it most likely that Click Fraud has happened?
#   Can you filter out certain OS in conjunction with certain devices at which Click Fraud comes up more likely?

ggplot(data=dataset, aes(x=os)) + geom_bar()

# V. channel
ggplot(data=dataset, aes(x=channel)) + geom_bar()

# VI. click_time

# Compare time distribution based on the click_time of fraudulent and natural clicks
ggplot(data=ds_is_attributed_1, aes(x=ds_is_attributed_1$bins_click_exact_time)) + geom_bar() + ggtitle("Distribution of Natural Clicks based on Time")
ggplot(data=ds_is_attributed_0, aes(x=ds_is_attributed_0$bins_click_exact_time)) + geom_bar() + ggtitle("Distribution of Fraudulent Clicks based on Time")

# 1. Which time does make it likely that Click Fraud has happened?
#   Is it more likely that Click Fraud has happenend during night times in comparison to during the day?
#   At which times of the day is it more likely that click fraud happens (morning, noon, afternoon, evening)?

min(dataset$click_date)
max(dataset$click_date)

#Dataset represents 4 days - Monday until Thursday


# VII. attributed_time


## Basic Statistics of the two sets
# Examine Data
str(dataset)
summary(dataset)

hist(dataset$ip)
hist(dataset$os)

# d. Check Correlations ------------------------------------------------------

# I. OS & Device

ct_os_device = table(dataset$os, dataset$device)

# Get the number of the most frequent combination in the dataset 
max(ct_os_device)

#Apply Chi-Square test to check interdependence 
chisq.test(table(dataset$os, dataset$device))

#Interpreation Results Chi-Squared Test: 
# If p-value here is lower than 0.05 (which is the case), then this means that both variables are dependent (null-hypothesis would test whether they are independent)
# The null hypothesis for this test is that there is no relationship between OS  and Device  The alternative hypothesis is that there is a relationship between OS and Device.

# Test which combination is most often observable in the dataset 
max(table(dataset$os, dataset$device))


# Try to visualize the relationship between two variables

# install.packages("ggpubr")
library(ggpubr)

#Balloonplot
ggballoonplot(data=as.data.frame(table(dataset$os, dataset$device)), x=ct_os_device$os, y=ct_os_device$device)

# Correspondence Analysis 
# install.packages("FactoMineR")
library(FactoMineR)
CA_os_device = CA(table(dataset$os, dataset$device), ncp=10)
MCA_os_device = MCA(table(dataset$os, dataset$device), ncp=10)

# II. Channel & Device 

# III. App &


# 4) Modeling --------------------------------------------------------------

## Method 1 - Naive Bayes from "e1071"-package
# install.packages('e1071')
library(e1071)
# install.packages("MLmetrics")
library(MLmetrics)
# install.packages("pROC")
library(pROC)

## CLASSIFIER: Try Naive Bayes classifiers with different inputs and compare performance

set.seed(213)

# 0) Excluding ip and including two bins
classifier_0 = naiveBayes(is_attributed ~ ip + app + device + os + channel + bins_click_date + bins_click_exact_time, data = training_set, laplace = 0.01)
y_pred_0 = predict(classifier_0, newdata = test_set[-9])

# 1) IP included and click_time excluded (only bins_click_time)
classifier_1 = naiveBayes(is_attributed ~ ip + app + device + os + channel + bins_click_time, data = training_set, laplace = 1)
y_pred_1 = predict(classifier_1, newdata = test_set[-9])

# 2)  Excluding OS and IP
classifier_2 = naiveBayes(is_attributed ~  app + device + channel + bins_click_time, data= training_set, laplace = 1)
# test_set$y_pred = predict(classifier, newdata = test_set)
y_pred_4 = predict(classifier_2, newdata = test_set[-9])

table(y_pred_2)
confusionMatrix(data = y_pred_2, reference = test_set$is_attributed)
F1_Score(y_true = test_set$is_attributed, y_pred = y_pred_2)
Precision(y_true = test_set$is_attributed, y_pred = y_pred_2)
Recall(y_true = test_set$is_attributed, y_pred = y_pred_2)


# 3) IP excluded
classifier_3 = naiveBayes(is_attributed ~ app + device + os + channel + bins_click_time, data= training_set)
y_pred_3 = predict(classifier_3, newdata = test_set[-9])

# 4) complete
classifier_4 = naiveBayes(is_attributed ~ ip + app + device + os + channel + bins_click_time, data= training_set)
y_pred_4 = predict(classifier_4, newdata = test_set[-9])

print(classifier)
summary(classifier) #Summary gives conditional probabilities across all features


#Insert correct classifier model object
y_pred = predict(classifier_ex_, newdata = test_set[-9])

#Get total number of 0 and 1 values
table(y_pred)  


## Method 2 - Naive Bayes from "naivebayes"-package

set.seed(987)

# Alternative Package
# install.packages("naivebayes")
library(naivebayes)

# Build a new model using the Laplace correction
classifier_2 <- naive_bayes(is_attributed ~ app + device + os + channel + bins_click_time, data=training_set, laplace=1)


# Observe the new predicted probabilities for a weekend afternoon
y_predict_2 <- predict(classifier_2, newdata=test_set[-8])

F1_Method_2 = F1_Score(y_true = test_set$is_attributed, y_pred = y_predict_2)
Accuracy(y_true = test_set$is_attributed, y_pred = y_predict_2)


# Additional Comments ___________________________________

# 5.  Inference: Which feature does help most to predict whether Click Fraud has happened or not?
#   Is it really one feature or a combination of different features?

# 7.  How fast does the Naive Bayes Classifier calculate the result? Is in that time a major advantage in comparison to other algorithms? What exactly do we have to measure in order to check the time - prediction function of the classifier on the data_forecast set?

# 9.  Kann man die Observations rausfiltern, bei denen die Wahrscheinlichkeit der Zuordnung zu einer Klasse nur marginal höher ist als die Zuordnung der anderen Klasse (z.B. 0.51 Klasse 1 vs. 0.49 für Klasse 2? Gibt es hierfür bestimmte Parameter, die man noch angeben kann?
#     •   P(Click Fraud | All Features) vs. P(Click Fraud | only a certain feature)
  

# 5) Model Performance -------------------------------------------------------

# CREATE the Confusion Matrix
# install.packages("caret")
library(caret)
# install.packages("ML)
confusionMatrix(data = y_pred, reference = test_set$is_attributed)

## Get F1-Score

F1_Method_1 = F1_Score(y_true = test_set$is_attributed, y_pred = y_pred)
F1_Method_1

Precision(y_true = test_set$is_attributed, y_pred = y_pred)
Recall(y_true = test_set$is_attributed, y_pred = y_pred)


apply(dataset)



# Precision = Precision(y_true = test_set$is_attributed, y_pred = i)
# , data_y = test_set$is_attributed

Precision()

#Model Performance Training Set


#Model Performance Test Set


get_model_performance = function (predict_variable, y_reference = test_set$is_attributed){
  library(MLmetrics)
  library(caret)
  require(MLmetrics)
  require(caret)
  
  Table_Predictions = table(predict_variable)
  Confusion_Matrix = confusionMatrix(data = predict_variable, reference = y_reference)
  Precision_Score = Precision(y_true = y_reference, y_pred = predict_variable)
  Recall_Score = Recall(y_true = y_reference, y_pred = predict_variable)
  F1 = F1_Score(y_true = y_reference, y_pred = predict_variable)
  Accuracy_Score = Accuracy(y_true = y_reference, y_pred = predict_variable)
  
  print(list(Table_Predictions=Table_Predictions, Confusion_Matrix=Confusion_Matrix, Precision_Score=Precision_Score, Recall_Score=Recall_Score, F1=F1, Accuracy_Score=Accuracy_Score))
}

#Get Performance for y_pred_0
get_model_performance(predict_variable = y_pred_0, y_reference = test_set$is_attributed)

?require

# c) ROC Curve
# e) CAP Curve



# 6) Performance Visualization ------------------------------------------------------

#https://datascienceplus.com/machine-learning-results-one-plot-to-rule-them-all/
# Check different plots to visualize classification results


# 7) Forecast -------------------------------------------------------------

#Apply same Data Preparation on "data_forecast" as on the basic "dataset"


# a. Apply binning --------------------------------------------------------

# i. Separation of "click_time"
#Separate blick_time variable into "click_date" and "click_exact_time"
data_forecast = separate(data_forecast, col=click_time, into = c("click_date", "click_exact_time"), sep= " ")
str(data_forecast)

# Convert variables "click_date" and "click_exact_time" into right formats -> Date and Time 
# install.packages("lubridate")
library(lubridate)
data_forecast$click_date = ymd(data_forecast$click_date) #choose this for date

#Apply binning on "click_date" and "click_exact_time"

# Binning on click_data
data_forecast$bins_click_date = NA
data_forecast$bins_click_date = cut(data_forecast$click_date, breaks="1 day", labels=FALSE) #works fine if previously converted
# Convert bin variable into factor
data_forecast$bins_click_date = as.factor(data_forecast$bins_click_date)
data_forecast$bins_click_date = factor(data_forecast$bins_click_date, levels=c("1","2","3","4"))

# Binning on click_exact_time - 12 intervals
data_forecast$bins_click_exact_time = NA
data_forecast$bins_click_exact_time <- cut(strptime(data_forecast$click_exact_time, format = "%H:%M:%S"), breaks=strptime(c("00:00:00","02:00:00","04:00:00","06:00:00","08:00:00","10:00:00","12:00:00","14:00:00","16:00:00","18:00:00","20:00:00","22:00:00"), format= "%H:%M:%S"), labels = c("0-2","2-4","4-6","6-8","8-10","10-12","12-14","14-16","16-18","18-20","20-22"))
data_forecast$bins_click_exact_time = as.character(data_forecast$bins_click_exact_time)
ind = which(is.na(data_forecast$bins_click_exact_time))
data_forecast$bins_click_exact_time[ind] = "22-24"
# Convert bin variable into factor
data_forecast$bins_click_exact_time = as.factor(data_forecast$bins_click_exact_time)
data_forecast$bins_click_exact_time = factor(data_forecast$bins_click_exact_time, levels=c("0-2","2-4","4-6","6-8","8-10","10-12","12-14","14-16","16-18","18-20","20-22","22-24"))


# b. Get Basic Statistics -------------------------------------------------




# c. Explore each Feature -------------------------------------------------




# (d. Check Correlations) -------------------------------------------------


# a) CART 
# install.packages("ranger")
library(ranger)

# b) Random Forest
# install.packages("randomForest")
library(randomForest)

randomForest(formula=is_attributed ~ app + device + os + channel + bins_click_time, data= training_set)
#Problem: 

# c) Boosted Trees - XGBOOST
# install.packages("xgboost")
library(xgboost)
#Problem: One-hot encoding wäre notwendig

# d) SVM
library(e1071)


#(7) Model Performance Comparison)  -------------------------------------------


# Sample Code  -------------------------------------------------------------

#Udemy

#Sample Code for Naive Bayes for DIFFERENT problem

# # Fitting SVM to the Training set

# classifier = naiveBayes(x = training_set[-3],
#                         y = training_set$Purchased)
# 
# # Predicting the Test set results
# y_pred = predict(classifier, newdata = test_set[-3])
# 
# # Making the Confusion Matrix
# cm = table(test_set[, 3], y_pred)
# 
# # Visualising the Training set results
# library(ElemStatLearn)
# set = training_set
# X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
# X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
# grid_set = expand.grid(X1, X2)
# colnames(grid_set) = c('Age', 'EstimatedSalary')
# y_grid = predict(classifier, newdata = grid_set)
# plot(set[, -3],
#      main = 'SVM (Training set)',
#      xlab = 'Age', ylab = 'Estimated Salary',
#      xlim = range(X1), ylim = range(X2))
# contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
# points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
# points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))
# 
# # Visualising the Test set results
# library(ElemStatLearn)
# set = test_set
# X1 = seq(min(set[, 1]) - 1, max(set[, 1]) + 1, by = 0.01)
# X2 = seq(min(set[, 2]) - 1, max(set[, 2]) + 1, by = 0.01)
# grid_set = expand.grid(X1, X2)
# colnames(grid_set) = c('Age', 'EstimatedSalary')
# y_grid = predict(classifier, newdata = grid_set)
# plot(set[, -3], main = 'SVM (Test set)',
#      xlab = 'Age', ylab = 'Estimated Salary',
#      xlim = range(X1), ylim = range(X2))
# contour(X1, X2, matrix(as.numeric(y_grid), length(X1), length(X2)), add = TRUE)
# points(grid_set, pch = '.', col = ifelse(y_grid == 1, 'springgreen3', 'tomato'))
# points(set, pch = 21, bg = ifelse(set[, 3] == 1, 'green4', 'red3'))


# Sample Code DataCamp

# building a Naive Bayes model
# install.packages("naivebayes")
library(naivebayes)

m <- naive_bayes(location ~ time_of_day, data = location_history)

# making predictions with Naive Bayes
future_location <- predict(m, future_conditions)

# The 'naivebayes' package is loaded into the workspace
# and the Naive Bayes 'locmodel' has been built

# Examine the location prediction model
locmodel

# Obtain the predicted probabilities for Thursday at 9am
predict(locmodel, thursday9am , type = "prob")

# Obtain the predicted probabilities for Saturday at 9am
predict(locmodel, saturday9am, type="prob")

# The 'naivebayes' package is loaded into the workspace already

# Build a NB model of location
locmodel <- naive_bayes(location ~ daytype + hourtype, data=locations)

# Predict Brett's location on a weekday afternoon
predict(locmodel, weekday_afternoon)

# Predict Brett's location on a weekday evening
predict(locmodel, newdata=weekday_evening)

#Great job! Your Naive Bayes model forecasts that Brett will be at the office on a weekday afternoon and at home in the evening.

?naive_bayes

# The 'naivebayes' package is loaded into the workspace already
# The Naive Bayes location model (locmodel) has already been built

# Observe the predicted probabilities for a weekend afternoon
predict(locmodel, weekend_afternoon, type = "prob")

# Build a new model using the Laplace correction
locmodel2 <- naive_bayes(location ~ daytype + hourtype, data=locations, laplace=1)

# Observe the new predicted probabilities for a weekend afternoon
predict(locmodel2, weekend_afternoon, type = "prob")


# Sample Code R for Marketing Book

# set.seed(04625) 
# train.prop <- 0.65 
# train.cases <- sample(nrow(seg.raw), nrow(seg.raw)*train.prop) 
# seg.df.train <- seg.raw[train.cases, ] 
# seg.df.test <- seg.raw[-train.cases, ]

library(e1071)
(initial_classified <- naiveBayes(is_attributed~., data=training_set))


# Sample Code R bloggers

# library (e1071)
# ?naiveBayes
# data ( "Titanic" )
# Titanic_df= as.data.frame (Titanic)
# repeating_sequence= rep.int ( seq_len ( nrow (Titanic_df)), Titanic_df$Freq)
# Titanic_dataset=Titanic_df[repeating_sequence,]
# Titanic_dataset$Freq= NULL
# Naive_Bayes_Model= naiveBayes (Survived ~., data=Titanic_dataset)
# Naive_Bayes_Model
# NB_Predictions= predict (Naive_Bayes_Model,Titanic_dataset)
# table (NB_Predictions,Titanic_dataset$Survived)
# library (mlr)
# task = makeClassifTask (data = Titanic_dataset, target = "Survived" )
# selected_model = makeLearner ( "classif.naiveBayes" )
# NB_mlr = train (selected_model, task)
# NB_mlr$learner.model
# predictions_mlr = as.data.frame ( predict (NB_mlr, newdata = Titanic_dataset[,1:3]))
# table(predictions_mlr[,1],Titanic_dataset$Survived)


# Non-used Code --------------------------------

# install.packages("descr")
# library(descr)
# CrossTable(dataset$os, dataset$device, chisq = TRUE)
# 
# # install.packages("gmodels")
# library(gmodels)
# CrossTable(dataset$os, dataset$device)

# b. Treat highly correlated variables 

# back_up = as.tibble(fread("train-all.csv", na.strings = ""))
# dataset = as.tibble(fread("train-all.csv", na.strings = ""))

# table(y_pred_3)
# confusionMatrix(data = y_pred_3, reference = test_set$is_attributed)
# F1_Score(y_true = test_set$is_attributed, y_pred = y_pred_3)
# Precision(y_true = test_set$is_attributed, y_pred = y_pred_3)
# Recall(y_true = test_set$is_attributed, y_pred = y_pred_3)
# Accuracy(y_true = test_set$is_attributed, y_pred = y_pred_3)

# table(y_pred_1)
# confusionMatrix(data = y_pred_1, reference = test_set$is_attributed)
# F1_Score(y_true = test_set$is_attributed, y_pred = y_pred_1)
# Precision(y_true = test_set$is_attributed, y_pred = y_pred_1)
# Recall(y_true = test_set$is_attributed, y_pred = y_pred_1)
 
table(y_pred_0)
confusionMatrix(data = y_pred_0, reference = test_set$is_attributed)
Precision(y_true = test_set$is_attributed, y_pred = y_pred_0)
Recall(y_true = test_set$is_attributed, y_pred = y_pred_0)
F1_Score(y_true = test_set$is_attributed, y_pred = y_pred_0)
Accuracy(y_true = test_set$is_attributed, y_pred = y_pred_0)

get_model_performance = function (predict_variable, y_reference = test_set$is_attributed){
  library(MLmetrics)
  library(caret)
  require(MLmetrics)
  require(caret)
  
  table(y_pred_0)
  confusionMatrix(data = y_pred_0, reference = test_set$is_attributed)
  Precision(y_true = test_set$is_attributed, y_pred = y_pred_0)
  Recall(y_true = test_set$is_attributed, y_pred = y_pred_0)
  F1_Score(y_true = test_set$is_attributed, y_pred = y_pred_0)
  Accuracy(y_true = test_set$is_attributed, y_pred = y_pred_0)
  
  print(list(Confusion_Matrix, Precision_Score, Recall_Score, F1, Accuracy_Score))
}

# get_model_performance = function(i){
#   table(i)
#   Confusion_Matrix = confusionMatrix(data = test_set$is_attributed, reference = test_set$is_attributed)
#   
#   Recall = Recall(y_true = test_set$is_attributed, y_pred = i)
#   F1 = F1_Score(y_true = test_set$is_attributed, y_pred = i)
#   Accuracy = Accuracy(y_true = test_set$is_attributed, y_pred = i)
#   list_performance = list(Confusion_Matrix=Confusion_Matrix, Precision= Precision, Recall=Recall, F1=F1, Accuracy=Accuracy)
# }

get_model_performance(i=y_pred_0)

# install.packages("Hmisc")
library(Hmisc)
#Get correlations with sig. convert to matrix first
rcorr(as.matrix(dataset$ip), as.matrix(dataset$channel))


# Check correlation between features
correlations = cor(training_set)
install.packages("xtable")
library(xtable)
#Check correlations between variables, which might by dependent 
# cor(dataset$device, dataset$os)
# cor(dataset$device, dataset$ip)
# cor(dataset$channel, dataset$ip)

# dataset$bins_click_exact_time = factor(dataset$bins_click_exact_time, levels=c("0-2","2-4","4-6","6-8","8-10","10-12","12-14","14-16","16-18","18-20","20-22","22-24"))

# dataset$bins_click_exact_time = factor(dataset$bins_click_exact_time, levels=c("0-2","2-4","4-6","6-8","8-10","10-12","12-14","14-16","16-18","18-20","20-22","22-24"))


# Data Preparation: Binning

# # ii. Separation of "attributed_time"
# We did not consider attributed time, since it is only related to the is_attributed = 1 meaning the Click Frauds, it does not help to differentiate between Click Fraud non-click-fraud. Therefore, we did not consider that as input variable for Naive Bayes.

# #Separate blick_time variable into "attributed_date" and "attributed_exact_time"
# dataset = separate(dataset, col=attributed_time, into = c("attributed_date", "attributed_exact_time"), sep= " ")
# 
# # Convert variables "attributed_date" and "attributed_exact_time" into right formats -> Date and Time 
# # install.packages("lubridate")
# dataset$attributed_date = ymd(dataset$attributed_date) #choose this for date
# 
# #Apply binning on "click_date" and "click_exact_time"
# # Binning on click_data
# dataset$bins_attributed_date = cut(dataset$attributed_date, breaks="1 day", labels=FALSE) #works fine if previously converted
# # Convert bin variable into factor
# dataset$bins_attributed_date = as.factor(dataset$bins_attributed_date)
# 
# # Binning on click_exact_time
# dataset$bins_attributed_exact_time <- cut(strptime(dataset$attributed_exact_time, format = "%H:%M:%S"), breaks=strptime(c("00:00:00","02:00:00","04:00:00","06:00:00","08:00:00","10:00:00","12:00:00","14:00:00","16:00:00","18:00:00","20:00:00","22:00:00"), format= "%H:%M:%S"), labels = c("0-2","2-4","4-6","6-8","8-10","10-12","12-14","14-16","16-18","18-20","20-22"))
# dataset$bins_attributed_exact_time = as.character(dataset$bins_attributed_exact_time)
# ind = which(is.na(dataset$bins_attributed_exact_time))
# dataset$bins_attributed_exact_time[ind] = "22-24"
# # Convert bin variable into factor
# dataset$bins_attributed_exact_time = as.factor(dataset$bins_attributed_exact_time)



# install.packages("lubridate")
library(lubridate)

# filter(FL_DATE >= as.Date("2014-01-05")) https://blog.exploratory.io/filter-with-date-function-ce8e84be680

min(dataset$click_time)
max(dataset$click_time)

#Subset different days in order to apply separate binning on them later
dataset_11_06 = subset(x=dataset, subset = dataset$click_time >= as.POSIXct("2017-11-06") & dataset$click_time < as.POSIXct("2017-11-07"))
dataset_11_07 = subset(x=dataset, subset = dataset$click_time >= as.POSIXct("2017-11-07") & dataset$click_time < as.POSIXct("2017-11-08"))
dataset_11_08 = subset(x=dataset, subset = dataset$click_time >= as.POSIXct("2017-11-08") & dataset$click_time < as.POSIXct("2017-11-09"))
dataset_11_09 = subset(x=dataset, subset = dataset$click_time >= as.POSIXct("2017-11-09") & dataset$click_time < as.POSIXct("2017-11-10"))

#Check whether subsetting has worked out correctly 
(nrow(dataset_11_06)+nrow(dataset_11_07)+nrow(dataset_11_08)+nrow(dataset_11_09)) == nrow(dataset)

dataset$click_exact_time <- cut(strptime(dataset$click_exact_time, format = "%H:%M:%S"), breaks=strptime(c("00:00:00","02:00:00","04:00:00","06:00:00","08:00:00","10:00:00","12:00:00","14:00:00","16:00:00","18:00:00","20:00:00","22:00:00"), format= "%H:%M:%S"), 
                                labels = c("0-2","2-4","4-6","6-8","8-10","10-12","12-14","14-16","16-18","18-20","20-22", "22-24"))

dataset$click_exact_time = hms(dataset$click_exact_time)

# dataset$bins_click_exact_time = cut(dataset$click_exact_time, breaks="2 hours", labels=FALSE)


dataset$bins_click_exact_time <- cut(strptime(dataset$click_exact_time, format = "%H:%M:%S"), breaks="2 hours", labels=FALSE)

# #works fine
# dataset$bins_click_exact_time = cut.Date(dataset$click_exact_time, breaks="2 hours", labels=FALSE) 



dataset$bins_click_exact_time = cut(dataset$click_exact_time, breaks="2 hours", labels=FALSE) #does not work, since numeric
dataset$bins_click_exact_time = cut.POSIXt(dataset$click_exact_time, breaks="2 hours", labels=FALSE) #does not work, since numeric


# To Do: Wichtig, dass die bins pro Tag gleich sind und sich nicht fortsetzen an den einzelnen Tagen, weil man ansonsten beim forecast_set Probleme bekommt
# Evtl. möglich, wenn man Datensatz nach Zeiten aufsteigend sortiert oder direkt spezifischen Tag anfiltert und dann für alle Tage manuell die bins setzt, sodass bins immer wieder beim gleichen Wert starten und bei dem gleichen Uhrzeitintervall


filter(dataset, attributed_time == 0)

str(dataset)
dataset2 = dataset

dataset2$attributed_time = as.character(dataset2$attributed_time)

dataset_attributed_1 = dataset[dataset$attributed_time == 1, ]
dataset_fraud_0 = dataset[dataset$attributed_time == 0, ]

library(ggplot2)
ggplot(data=dataset, aes(x=bins_click_exact_time)) + geom_bar()
ggplot(data=dataset, aes(x=))

summary(dataset)
str(dataset)
library(dplyr)